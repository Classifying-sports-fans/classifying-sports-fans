{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e799c4",
   "metadata": {},
   "source": [
    "## Exloring various classification models\n",
    "\n",
    "We explore various classification models, finding one that performs the best again the baseline model of a coin flip. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9f7839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## this is to suppress warnings I was getting in this code. \n",
    "import warnings\n",
    "# Suppress FutureWarning messages\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bf7661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df = pd.read_csv('survey_data_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cb2e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement = True\n",
    "estimators = 100\n",
    "samples = .25\n",
    "\n",
    "## the models we will run for the training survey data\n",
    "\n",
    "models = {'slc': LogisticRegression(penalty=None, max_iter=300),\n",
    "          'svc': SVC(),\n",
    "          'knn': Pipeline((('scale', StandardScaler()), ('knnc',KNeighborsClassifier()))),\n",
    "         'ada': AdaBoostClassifier(),\n",
    "         'gbc': GradientBoostingClassifier(),\n",
    "         'rfc': RandomForestClassifier(),\n",
    "          'etc': ExtraTreesClassifier(),\n",
    "         'xgbc': XGBClassifier()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "660265a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#possible features we want to include\n",
    "features = ['S1','S2', 'Hid_Ethnicity_Buckets','D4','Fan_magnitude']\n",
    "\n",
    "## target VL1 responses we want to predict\n",
    "targets = ['VL1r1','VL1r2','VL1r4','VL1r5','VL1r7',\n",
    "           'VL1r10','VL1r11','VL1r12','VL1r13' ,'VL1r14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15e591df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on value question:  VL1r1\n",
      "Working on value question:  VL1r2\n",
      "Working on value question:  VL1r4\n",
      "Working on value question:  VL1r5\n",
      "Working on value question:  VL1r7\n",
      "Working on value question:  VL1r10\n",
      "Working on value question:  VL1r11\n",
      "Working on value question:  VL1r12\n",
      "Working on value question:  VL1r13\n",
      "Working on value question:  VL1r14\n"
     ]
    }
   ],
   "source": [
    "## create a dictionary that will keep the accuracies of all the VL1's\n",
    "all_accuracy = {}\n",
    "\n",
    "## initiate our cross validating sets\n",
    "kfold = KFold(n_splits=5, \n",
    "                        shuffle=True, \n",
    "                        random_state=5555)\n",
    "\n",
    "## initiate the diciontaries to keep track of feature importance\n",
    "\n",
    "xgbc_importance ={} ## feature importance for XGBoost\n",
    "\n",
    "ada_importance ={} ## feature importance for AdaBoost\n",
    "\n",
    "gbc_importance ={} ## feature importance for GradientBoost\n",
    "\n",
    "\n",
    "## start to work through the models, first taking out a target VL1\n",
    "for target in targets:\n",
    "    print('Working on value question: ', target)\n",
    "\n",
    "    accuracy = {} # initiate dictionary of accuracies for this specific VL1\n",
    "    \n",
    "    for name, model in models.items(): # grab one specific model\n",
    "        accu = np.zeros((1,5))\n",
    "        for i, (train_index, test_index) in enumerate(kfold.split(survey_df)):\n",
    "            fans_tt = survey_df.iloc[train_index] ## cv training set\n",
    "            fans_ho = survey_df.iloc[test_index] ## cv hold out set\n",
    "    \n",
    "            model.fit(fans_tt[features].values, fans_tt[target].values) ## fitting the model\n",
    "        \n",
    "            pred = model.predict(fans_ho[features].values) ## predicting \n",
    "        \n",
    "            accu[0,i] = accuracy_score(fans_ho[target].values, pred) ## storing accuracies in a numpy array\n",
    "        \n",
    "        ## grabbing each models feature importances\n",
    "        \n",
    "        if name == 'xgbc': \n",
    "            xgbc_importance[target] = model.feature_importances_\n",
    "        if name == 'ada':\n",
    "            ada_importance[target] = model.feature_importances_\n",
    "        if name == 'gbc':\n",
    "            gbc_importance[target] = model.feature_importances_\n",
    "            \n",
    "        ## storing the average of the numpy array of accuracies for specifc model used\n",
    "        accuracy[name] = accu.mean()\n",
    "    ## storing the dictionary of average accuracies, with key VL1 \n",
    "    all_accuracy[target] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef133971",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For question  VL1r1\n",
      "here are the accuracy scores\n",
      "{'slc': 0.6537597923201857, 'svc': 0.6355422366492848, 'knn': 0.6179273076970675, 'ada': 0.6607569014882214, 'gbc': 0.65918787350226, 'rfc': 0.6193750532339681, 'etc': 0.6065872531126857, 'xgbc': 0.6367471260937079}\n",
      "------\n",
      "For question  VL1r2\n",
      "here are the accuracy scores\n",
      "{'slc': 0.6380745355632026, 'svc': 0.6348179634885958, 'knn': 0.6022436525090398, 'ada': 0.6438650809192715, 'gbc': 0.6449497070948776, 'rfc': 0.6087556318808317, 'etc': 0.6042910400224802, 'xgbc': 0.611411761194421}\n",
      "------\n",
      "For question  VL1r4\n",
      "here are the accuracy scores\n",
      "{'slc': 0.7999762676600277, 'svc': 0.7947881306268758, 'knn': 0.7740374752575796, 'ada': 0.7998551308081444, 'gbc': 0.7984077492641882, 'rfc': 0.77886424009557, 'etc': 0.7647486665118484, 'xgbc': 0.7868281145238281}\n",
      "------\n",
      "For question  VL1r5\n",
      "here are the accuracy scores\n",
      "{'slc': 0.6508633548647292, 'svc': 0.6497784374947677, 'knn': 0.6118966725948984, 'ada': 0.6477274100518835, 'gbc': 0.6515871912338851, 'rfc': 0.6018814795294009, 'etc': 0.5900589886965631, 'xgbc': 0.6187730089039954}\n",
      "------\n",
      "For question  VL1r7\n",
      "here are the accuracy scores\n",
      "{'slc': 0.6094818706034203, 'svc': 0.5940384507586705, 'knn': 0.5603819159569411, 'ada': 0.6081535875508589, 'gbc': 0.6063441786244416, 'rfc': 0.5714815896008671, 'etc': 0.5555559519034283, 'xgbc': 0.5820961334485493}\n",
      "------\n",
      "For question  VL1r10\n",
      "here are the accuracy scores\n",
      "{'slc': 0.7300019728417585, 'svc': 0.7191440633114767, 'knn': 0.6933277909341005, 'ada': 0.7252981648203731, 'gbc': 0.7234885374981892, 'rfc': 0.6883812724174154, 'etc': 0.668595198350675, 'xgbc': 0.7075630453979281}\n",
      "------\n",
      "For question  VL1r11\n",
      "here are the accuracy scores\n",
      "{'slc': 0.7723502951618786, 'svc': 0.7522013929281994, 'knn': 0.7430325562569294, 'ada': 0.7727113761626845, 'gbc': 0.7730735491423234, 'rfc': 0.7411041216377062, 'etc': 0.7345899583082482, 'xgbc': 0.7511160387667045}\n",
      "------\n",
      "For question  VL1r12\n",
      "here are the accuracy scores\n",
      "{'slc': 0.9188082434210095, 'svc': 0.919049425145943, 'knn': 0.9121730160382571, 'ada': 0.9185669888974873, 'gbc': 0.9163955525886086, 'rfc': 0.9061413617558436, 'etc': 0.8982996433597131, 'xgbc': 0.9100014341322009}\n",
      "------\n",
      "For question  VL1r13\n",
      "here are the accuracy scores\n",
      "{'slc': 0.7829651302039162, 'svc': 0.7813966118080766, 'knn': 0.7470132558950476, 'ada': 0.7818791936537102, 'gbc': 0.7842921028818778, 'rfc': 0.7439971375594855, 'etc': 0.7310884189820864, 'xgbc': 0.7602844386464412}\n",
      "------\n",
      "For question  VL1r14\n",
      "here are the accuracy scores\n",
      "{'slc': 0.8432870601236265, 'svc': 0.8395458678428979, 'knn': 0.825550994319526, 'ada': 0.8428041142850488, 'gbc': 0.8413562231509705, 'rfc': 0.8248281043320256, 'etc': 0.8162614575879061, 'xgbc': 0.8272402127757156}\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# printing out the dictionary for each VL\n",
    "for value, scores in all_accuracy.items():\n",
    "    print('For question ', value)\n",
    "    print('here are the accuracy scores')\n",
    "    print(scores)\n",
    "    print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8251d376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  VL1r1\n",
      "Adaboost score was  0.6607569014882214\n",
      "The best performing model was:  ada\n",
      "with accuracy score  0.6607569014882214\n",
      "-----\n",
      "For  VL1r2\n",
      "Adaboost score was  0.6438650809192715\n",
      "The best performing model was:  gbc\n",
      "with accuracy score  0.6449497070948776\n",
      "-----\n",
      "For  VL1r4\n",
      "Adaboost score was  0.7998551308081444\n",
      "The best performing model was:  slc\n",
      "with accuracy score  0.7999762676600277\n",
      "-----\n",
      "For  VL1r5\n",
      "Adaboost score was  0.6477274100518835\n",
      "The best performing model was:  gbc\n",
      "with accuracy score  0.6515871912338851\n",
      "-----\n",
      "For  VL1r7\n",
      "Adaboost score was  0.6081535875508589\n",
      "The best performing model was:  slc\n",
      "with accuracy score  0.6094818706034203\n",
      "-----\n",
      "For  VL1r10\n",
      "Adaboost score was  0.7252981648203731\n",
      "The best performing model was:  slc\n",
      "with accuracy score  0.7300019728417585\n",
      "-----\n",
      "For  VL1r11\n",
      "Adaboost score was  0.7727113761626845\n",
      "The best performing model was:  gbc\n",
      "with accuracy score  0.7730735491423234\n",
      "-----\n",
      "For  VL1r12\n",
      "Adaboost score was  0.9185669888974873\n",
      "The best performing model was:  svc\n",
      "with accuracy score  0.919049425145943\n",
      "-----\n",
      "For  VL1r13\n",
      "Adaboost score was  0.7818791936537102\n",
      "The best performing model was:  gbc\n",
      "with accuracy score  0.7842921028818778\n",
      "-----\n",
      "For  VL1r14\n",
      "Adaboost score was  0.8428041142850488\n",
      "The best performing model was:  slc\n",
      "with accuracy score  0.8432870601236265\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "## for each VL1, we want to print out the better performing model\n",
    "\n",
    "for VL in all_accuracy.items():\n",
    "    print('For ', VL[0])\n",
    "    \n",
    "    for i, value in enumerate(VL[1].items()):\n",
    "        if i == 0:\n",
    "            name = value[0]\n",
    "            score = value[1]\n",
    "        if i > 0:\n",
    "            if value[1] > score:\n",
    "                name = value[0]\n",
    "                score = value[1]\n",
    "        if value[0] == 'ada':\n",
    "            print('Adaboost score was ', value[1])\n",
    "    print('The best performing model was: ', name)\n",
    "    print('with accuracy score ', score)\n",
    "    print('-----')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_may_2024_new",
   "language": "python",
   "name": "erdos_may_2024_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
